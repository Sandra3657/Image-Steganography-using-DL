{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5kiA4Y9snVon"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torch\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGG4-cDr9y2E",
        "outputId": "36643be1-1f91-40df-c41e-ba1d3f3a12d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=37c4f156164a294a42dc2ebb8f02f59012c3eeef6231b8e8ebb4dbbedd361669\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "# %pip install poutyne          # to install the Poutyne library\n",
        "%pip install wget             # to install the wget library in order to download data\n",
        "# %pip install opencv-python    # to install the cv2 (opencv) library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "35vO_qom99P_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import wget\n",
        "import zipfile\n",
        "# import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as tfms\n",
        "# from poutyne import set_seeds, Model, ModelCheckpoint, CSVLogger, Experiment, StepLR\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ghC_tateREV_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj03dZZ4nVou",
        "outputId": "98f49c88-ab56-4722-e63a-a50543ac98e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2717d6ce10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wA6QM_LGnVov"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "lr = 1e-3\n",
        "dataset_location =\"./datasets/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "def get_celeba(batch_size, dataset_directory, train_val_split=0.8):\n",
        "    # 1. Download this file into dataset_directory and unzip it:\n",
        "    #  https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n",
        "    # 2. Put the `img_align_celeba` directory into the `celeba` directory!\n",
        "    # 3. Dataset directory structure should look like this (required by ImageFolder from torchvision):\n",
        "    #  +- `dataset_directory`\n",
        "    #     +- celeba\n",
        "    #        +- img_align_celeba\n",
        "    #           +- 000001.jpg\n",
        "    #           +- 000002.jpg\n",
        "    #           +- 000003.jpg\n",
        "    #           +- ...\n",
        "    train_transformation = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "    dataset = torchvision.datasets.ImageFolder(dataset_directory + 'celeba', train_transformation)\n",
        "    dataset = torch.utils.data.Subset(dataset, np.random.choice(len(dataset), 50000, replace=False))\n",
        "    \n",
        "    n = len(dataset)\n",
        "    # test_split = train_val_split/2.0\n",
        "\n",
        "    # n_test = int(n*test_split)\n",
        "    # n_train = int((n-2*n_test)/2)\n",
        "    n_test = int(5000/2)\n",
        "    n_train_val = int((n-5000))\n",
        "    n_train = int(train_val_split*n_train_val/2)\n",
        "    n_val = int((n_train_val-(2*n_train))/2)\n",
        "    print(\"No. of training imgs:\", 2*n_train)\n",
        "    print(\"No. of validation imgs:\", 2*n_val)\n",
        "    print(\"No. of testing imgs:\", 2*n_test)\n",
        "    source_train, payload_train, source_test, payload_test, source_val, payload_val = torch.utils.data.random_split(\n",
        "    dataset, (n_train, n_train, n_test, n_test, n_val, n_val))\n",
        "\n",
        "    # Prepare Data Loaders for training and validation\n",
        "    source_train_loader = torch.utils.data.DataLoader(source_train, batch_size=batch_size, shuffle=True)\n",
        "    source_test_loader = torch.utils.data.DataLoader(source_test, shuffle=True)\n",
        "    source_val_loader = torch.utils.data.DataLoader(source_val, shuffle=True)\n",
        "    payload_train_loader = torch.utils.data.DataLoader(payload_train, batch_size=batch_size, shuffle=True)\n",
        "    payload_test_loader = torch.utils.data.DataLoader(payload_test, shuffle=True)\n",
        "    payload_val_loader = torch.utils.data.DataLoader(payload_val, shuffle=True)\n",
        "    \n",
        "\n",
        "\n",
        "    return (source_train_loader, payload_train_loader) , (source_test_loader, payload_test_loader), (source_val_loader, payload_val_loader)\n"
      ],
      "metadata": {
        "id": "sbwVoEVlRf1y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = \"datasets\"\n",
        "\n",
        "base_url = \"https://graal.ift.ulaval.ca/public/celeba/\"\n",
        "\n",
        "file_list = [\n",
        "    \"img_align_celeba.zip\",\n",
        "]\n",
        "\n",
        "# Path to folder with the dataset\n",
        "dataset_folder = f\"{data_root}/celeba\"\n",
        "os.makedirs(dataset_folder, exist_ok=True)\n",
        "\n",
        "for file in file_list:\n",
        "    url = f\"{base_url}/{file}\"\n",
        "    if not os.path.exists(f\"{dataset_folder}/{file}\"):\n",
        "        wget.download(url, f\"{dataset_folder}/{file}\")\n",
        "\n",
        "with zipfile.ZipFile(f\"{dataset_folder}/img_align_celeba.zip\", \"r\") as ziphandler:\n",
        "    ziphandler.extractall(dataset_folder)"
      ],
      "metadata": {
        "id": "kt1DglzWRv9X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rSXDSElRnVox",
        "outputId": "508169fa-3794-4379-a000-1b15ad4701bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training imgs: 36000\n",
            "No. of validation imgs: 9000\n",
            "No. of testing imgs: 5000\n"
          ]
        }
      ],
      "source": [
        "train_loader, test_loader, val_loader = get_celeba(batch_size, dataset_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GxSISjh-nVoy"
      },
      "outputs": [],
      "source": [
        "s_train, p_train = train_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0Kg0RDE5nVoz",
        "outputId": "1c9984ef-2f7e-4b23-b9a3-e60a6ad1ba95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "282"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(s_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7gUEFNHFndx2"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "    self.secret_conv1 = nn.Conv2d(3,8, kernel_size=3, padding=\"same\")\n",
        "    self.secret_conv2 = nn.Conv2d(8,16, kernel_size=3, padding=\"same\")\n",
        "    self.secret_conv3 = nn.Conv2d(16,32, kernel_size=3, padding=\"same\")\n",
        "\n",
        "    self.cover_conv1 = nn.Conv2d(3,8, kernel_size=3, padding=\"same\")\n",
        "    self.cover_conv2 = nn.Conv2d(8,16, kernel_size=3, padding=\"same\")\n",
        "    self.cover_conv3 = nn.Conv2d(16,32, kernel_size=3, padding=\"same\")\n",
        "\n",
        "    self.conv1 = nn.Conv2d(64,64, kernel_size=3, padding=\"same\")\n",
        "    self.conv2 = nn.Conv2d(64,128, kernel_size=3, padding=\"same\")\n",
        "    self.conv3 = nn.Conv2d(128,128, kernel_size=3, padding=\"same\")\n",
        "    self.conv4 = nn.Conv2d(128,64, kernel_size=3, padding=\"same\")\n",
        "    self.conv5 = nn.Conv2d(64,32, kernel_size=3, padding=\"same\")\n",
        "    self.conv6 = nn.Conv2d(32,16, kernel_size=3, padding=\"same\")\n",
        "    self.conv7 = nn.Conv2d(16,8, kernel_size=3, padding=\"same\")\n",
        "    self.conv8 = nn.Conv2d(8,3, kernel_size=3, padding=\"same\")\n",
        "\n",
        "  def forward(self,s,p):\n",
        "    s = F.relu(self.cover_conv1(s))\n",
        "    s = F.relu(self.cover_conv2(s))\n",
        "    s = F.relu(self.cover_conv3(s))\n",
        "\n",
        "    p = F.relu(self.secret_conv1(p))\n",
        "    p = F.relu(self.secret_conv2(p))\n",
        "    p = F.relu(self.secret_conv3(p))\n",
        "\n",
        "    m = torch.cat((s, p), 1)\n",
        "\n",
        "    m = F.relu(self.conv1(m))\n",
        "    m = F.relu(self.conv2(m))\n",
        "    m = F.relu(self.conv3(m))\n",
        "    m = F.relu(self.conv4(m))\n",
        "    m = F.relu(self.conv5(m))\n",
        "    m = F.relu(self.conv6(m))\n",
        "    m = F.relu(self.conv7(m))\n",
        "    m = F.relu(self.conv8(m))\n",
        "\n",
        "    return m\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3,8, kernel_size=3, padding=\"same\")\n",
        "    self.conv2 = nn.Conv2d(8,16, kernel_size=3, padding=\"same\")\n",
        "    self.conv3 = nn.Conv2d(16,32, kernel_size=3, padding=\"same\")\n",
        "    self.conv4 = nn.Conv2d(32,64, kernel_size=3, padding=\"same\")\n",
        "    self.conv5 = nn.Conv2d(64,128, kernel_size=3, padding=\"same\")\n",
        "    self.conv6 = nn.Conv2d(128,128, kernel_size=3, padding=\"same\")\n",
        "    self.conv7 = nn.Conv2d(128,64, kernel_size=3, padding=\"same\")\n",
        "    self.conv8 = nn.Conv2d(64,32, kernel_size=3, padding=\"same\")\n",
        "    self.conv9 = nn.Conv2d(32,16, kernel_size=3, padding=\"same\")\n",
        "    self.conv10 = nn.Conv2d(16,8, kernel_size=3, padding=\"same\")\n",
        "    self.conv11 = nn.Conv2d(8,3, kernel_size=3, padding=\"same\")\n",
        "  \n",
        "  def forward(self, x):\n",
        "    o = F.relu(self.conv1(x))\n",
        "    o = F.relu(self.conv2(o))\n",
        "    o = F.relu(self.conv3(o))\n",
        "    o = F.relu(self.conv4(o))\n",
        "    o = F.relu(self.conv5(o))\n",
        "    o = F.relu(self.conv6(o))\n",
        "    o = F.relu(self.conv7(o))\n",
        "    o = F.relu(self.conv8(o))\n",
        "    o = F.relu(self.conv9(o))\n",
        "    o = F.relu(self.conv10(o))\n",
        "    o = F.relu(self.conv11(o))\n",
        "    return o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uMKy8FxTnVoz"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder()\n",
        "decoder = Decoder()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)"
      ],
      "metadata": {
        "id": "VnO5MbZFR3Ry"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MUeHRIddnVo0"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXCkf8YVnVo1",
        "outputId": "0d7b4cc2-b910-408a-dda1-ba3bbda6770e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "0 30.615562438964844\n",
            "10 343.30056953430176\n",
            "20 654.482723236084\n",
            "30 963.9794368743896\n",
            "40 1277.5121994018555\n",
            "50 1590.60111618042\n",
            "60 1902.8192138671875\n",
            "70 2215.7563877105713\n",
            "80 2526.1667346954346\n",
            "90 2841.602746963501\n",
            "100 3151.1273307800293\n",
            "110 3467.084041595459\n",
            "120 3778.2635173797607\n",
            "130 4090.0093955993652\n",
            "140 4402.585229873657\n",
            "150 4707.414125442505\n",
            "160 5016.969457626343\n",
            "170 5324.481563568115\n",
            "180 5639.997678756714\n",
            "190 5956.826593399048\n",
            "200 6271.01487159729\n",
            "210 6579.390022277832\n",
            "220 6889.071422576904\n",
            "230 7198.269582748413\n",
            "240 7508.9098834991455\n",
            "250 7825.1611614227295\n",
            "260 8135.097850799561\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0\n",
        "  print(\"Epoch:\", epoch+1)\n",
        "  for i, (s,p) in enumerate(zip(s_train,p_train)):\n",
        "    s, _ = s  #cover\n",
        "    p, _ = p  #secret'\n",
        "    s = s.to(device)\n",
        "    p = p.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    stegono_img = encoder(s,p)\n",
        "    output = decoder(stegono_img)\n",
        "\n",
        "    loss1 = criterion(stegono_img, s)\n",
        "    loss2 = criterion(output, p)\n",
        "\n",
        "    loss = loss1 + 0.3*loss2\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()* s.size(0)\n",
        "    if i%10 == 0:\n",
        "      print(i, train_loss)\n",
        "  train_loss = train_loss/len(train_loader)\n",
        "  losses.append(train_loss)\n",
        "  print('epoch [{}/{}], loss:{:.6f}'.format(epoch + 1, num_epochs, train_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYxPJ_hvnVo2",
        "outputId": "58aa45c9-eb80-4ca0-8965-a8bf848c3127"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doYedJjvnVo3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "a6f808e8670a7dd0ff393c667e8220dfb4b5dd4ac295f8f01ea5d3e6de9ed98d"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}